<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Executive summary</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>



<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<p>Author: Rainer-Anton Englisch</p>

<h2>Executive summary</h2>

<p>&ldquo;Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a> (see the section on the Weight Lifting Exercise Dataset).&rdquo;</p>

<p>Simply put: We want to build a prediction model to predict activity quality from activity monitors. </p>

<h2>Predictor Analysis and Reduction</h2>

<p>Before letting caret to create a prediction model we will try to reduce the number of predictors in order to speed up the creation of the prediction model. First we load the training data set and remove the column &ldquo;X&rdquo;&ldquo; to avoid overfitting by observation number.</p>

<pre><code class="r">set.seed(1312)
pml_training = read.csv(&quot;pml-training.csv&quot;)
pml_training = pml_training[,-1]
dim(pml_training)
</code></pre>

<pre><code>## [1] 19622   159
</code></pre>

<p>Next we split the original training in a new training and a test data. The test data will be used later for estimating the out of sample error.</p>

<p>The activity quality of an observation is classified by the factor variable classe which we store in seperate variables for later use for training and prediction.</p>

<pre><code class="r">library(caret)
inTrain &lt;- createDataPartition(pml_training$classe,p=0.75, list=FALSE)
training &lt;- pml_training[inTrain,]
trainclasse &lt;- training$classe
testing &lt;- pml_training[-inTrain,]
testingclasse &lt;- testing$classe
</code></pre>

<p>Next we throw out all predictors that have near zero variability because we assume
that these predictors have minimal influence on the prediction.</p>

<pre><code class="r">nZV &lt;- nearZeroVar(training,saveMetrics=TRUE)
# extract the predictor names which have near zero variability
nzvcolnames &lt;- rownames(nZV[nZV$nzv==TRUE,])
# compute the index of these predictor  names in the training data frame
nzvcolindex &lt;- which(names(training) %in% nzvcolnames)
# remove these predictors
training &lt;- training[,-nzvcolindex]
# print these predictors
nzvcolnames
</code></pre>

<pre><code>##  [1] &quot;new_window&quot;              &quot;kurtosis_roll_belt&quot;     
##  [3] &quot;kurtosis_picth_belt&quot;     &quot;kurtosis_yaw_belt&quot;      
##  [5] &quot;skewness_roll_belt&quot;      &quot;skewness_roll_belt.1&quot;   
##  [7] &quot;skewness_yaw_belt&quot;       &quot;max_yaw_belt&quot;           
##  [9] &quot;min_yaw_belt&quot;            &quot;amplitude_yaw_belt&quot;     
## [11] &quot;avg_roll_arm&quot;            &quot;stddev_roll_arm&quot;        
## [13] &quot;var_roll_arm&quot;            &quot;avg_pitch_arm&quot;          
## [15] &quot;stddev_pitch_arm&quot;        &quot;var_pitch_arm&quot;          
## [17] &quot;avg_yaw_arm&quot;             &quot;stddev_yaw_arm&quot;         
## [19] &quot;var_yaw_arm&quot;             &quot;kurtosis_roll_arm&quot;      
## [21] &quot;kurtosis_picth_arm&quot;      &quot;kurtosis_yaw_arm&quot;       
## [23] &quot;skewness_roll_arm&quot;       &quot;skewness_pitch_arm&quot;     
## [25] &quot;skewness_yaw_arm&quot;        &quot;amplitude_pitch_arm&quot;    
## [27] &quot;kurtosis_roll_dumbbell&quot;  &quot;kurtosis_picth_dumbbell&quot;
## [29] &quot;kurtosis_yaw_dumbbell&quot;   &quot;skewness_roll_dumbbell&quot; 
## [31] &quot;skewness_pitch_dumbbell&quot; &quot;skewness_yaw_dumbbell&quot;  
## [33] &quot;max_yaw_dumbbell&quot;        &quot;min_yaw_dumbbell&quot;       
## [35] &quot;amplitude_yaw_dumbbell&quot;  &quot;kurtosis_roll_forearm&quot;  
## [37] &quot;kurtosis_picth_forearm&quot;  &quot;kurtosis_yaw_forearm&quot;   
## [39] &quot;skewness_roll_forearm&quot;   &quot;skewness_pitch_forearm&quot; 
## [41] &quot;skewness_yaw_forearm&quot;    &quot;max_roll_forearm&quot;       
## [43] &quot;max_yaw_forearm&quot;         &quot;min_roll_forearm&quot;       
## [45] &quot;min_yaw_forearm&quot;         &quot;amplitude_roll_forearm&quot; 
## [47] &quot;amplitude_yaw_forearm&quot;   &quot;avg_roll_forearm&quot;       
## [49] &quot;stddev_roll_forearm&quot;     &quot;var_roll_forearm&quot;       
## [51] &quot;avg_pitch_forearm&quot;       &quot;stddev_pitch_forearm&quot;   
## [53] &quot;var_pitch_forearm&quot;       &quot;avg_yaw_forearm&quot;        
## [55] &quot;stddev_yaw_forearm&quot;      &quot;var_yaw_forearm&quot;
</code></pre>

<p>We removed 56 predictors in the training data frame.</p>

<p>Next we want to throw out predictors that have a great linear correlation.</p>

<pre><code class="r">#As the correlation matix can only be computed for numeric variables we need to identify numeric variables
colsnumeric &lt;- sapply(training, is.numeric)

#compute the correlation matrix
cortraining &lt;- cor(training[,colsnumeric],use=&quot;pairwise.complete.obs&quot;)
</code></pre>

<p>Within the computed correlation matrix we select all predictors that have a high correlation. Let us define a high corelation as a value equal or greater than 0.7. Thus let&#39;s find these predictors and remove them from the training data set.</p>

<pre><code class="r"># retrieve variables that have a correlation greater or equal to 0.7
highlyCor &lt;- findCorrelation(cortraining, 0.70,verbose=FALSE)
highlyCorcolnames &lt;- colnames(training)[highlyCor]
# remove the highly correlated predictors from the training data frame
training &lt;- training[,-highlyCor]
# print the removed predictors
highlyCorcolnames
</code></pre>

<pre><code>##  [1] &quot;yaw_belt&quot;                &quot;stddev_pitch_belt&quot;      
##  [3] &quot;max_roll_belt&quot;           &quot;roll_belt&quot;              
##  [5] &quot;yaw_dumbbell&quot;            &quot;amplitude_pitch_belt&quot;   
##  [7] &quot;max_picth_belt&quot;          &quot;total_accel_belt&quot;       
##  [9] &quot;accel_belt_y&quot;            &quot;accel_belt_x&quot;           
## [11] &quot;cvtd_timestamp&quot;          &quot;stddev_pitch_dumbbell&quot;  
## [13] &quot;pitch_belt&quot;              &quot;gyros_belt_y&quot;           
## [15] &quot;stddev_roll_belt&quot;        &quot;gyros_belt_z&quot;           
## [17] &quot;accel_dumbbell_x&quot;        &quot;roll_dumbbell&quot;          
## [19] &quot;pitch_dumbbell&quot;          &quot;gyros_dumbbell_y&quot;       
## [21] &quot;amplitude_roll_dumbbell&quot; &quot;gyros_dumbbell_z&quot;       
## [23] &quot;min_pitch_dumbbell&quot;      &quot;min_roll_dumbbell&quot;      
## [25] &quot;var_accel_dumbbell&quot;      &quot;var_pitch_dumbbell&quot;     
## [27] &quot;stddev_roll_dumbbell&quot;    &quot;var_roll_dumbbell&quot;      
## [29] &quot;accel_dumbbell_y&quot;        &quot;accel_dumbbell_z&quot;       
## [31] &quot;user_name&quot;               &quot;max_picth_forearm&quot;      
## [33] &quot;gyros_arm_y&quot;             &quot;accel_belt_z&quot;           
## [35] &quot;total_accel_dumbbell&quot;    &quot;max_roll_arm&quot;           
## [37] &quot;accel_arm_x&quot;             &quot;amplitude_roll_belt&quot;    
## [39] &quot;min_pitch_belt&quot;          &quot;var_total_accel_belt&quot;   
## [41] &quot;accel_arm_z&quot;             &quot;min_yaw_arm&quot;            
## [43] &quot;gyros_forearm_z&quot;         &quot;var_roll_belt&quot;          
## [45] &quot;gyros_dumbbell_x&quot;        &quot;var_accel_arm&quot;          
## [47] &quot;stddev_yaw_dumbbell&quot;     &quot;var_accel_forearm&quot;      
## [49] &quot;min_roll_belt&quot;           &quot;var_pitch_belt&quot;
</code></pre>

<p>We removed 50 highly correlated predictors.</p>

<p>Now we want to quickly create  a small prediction model in order to
query the importance of the variables for the prediction model.
Let&#39;s fit a model, print the important predictors and keep these predictors in the training data set.</p>

<pre><code class="r">modFit &lt;- train(classe ~.,data=training,method=&quot;rpart&quot;)
</code></pre>

<pre><code>## Loading required package: rpart
## Loading required namespace: e1071
</code></pre>

<pre><code class="r"># print summary of the model fit
modFit
</code></pre>

<pre><code>## CART 
## 
## 14718 samples
##    52 predictors
##     5 classes: &#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## 
## Summary of sample sizes: 302, 302, 302, 302, 302, 302, ... 
## 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa      Accuracy SD  Kappa SD  
##   0.03587444  0.4927385  0.3601873  0.05738967   0.07025669
##   0.08968610  0.3911354  0.1996817  0.04463675   0.05431438
##   0.15246637  0.2985085  0.0647217  0.07264218   0.07784641
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was cp = 0.03587444.
</code></pre>

<pre><code class="r"># retrieve the variable importance list
importance &lt;- varImp(modFit, scale=FALSE)
importance &lt;- importance[[1]]
# bind the rownames as columns 
importance &lt;- cbind(rownames(importance),importance )
# retrieve column names which have importance greater zero
impcolnames &lt;- importance[importance$Overall&gt;0.0,1]
# print important variables
impcolnames
</code></pre>

<pre><code>##  [1] amplitude_yaw_arm avg_roll_belt     avg_roll_dumbbell
##  [4] magnet_belt_y     magnet_dumbbell_y magnet_dumbbell_z
##  [7] num_window        pitch_forearm     roll_arm         
## [10] roll_forearm      stddev_yaw_belt   var_yaw_belt     
## 52 Levels: accel_arm_y accel_forearm_x accel_forearm_y ... yaw_forearm
</code></pre>

<pre><code class="r"># retrieve indexes of column names
impcolindex &lt;- which(names(training) %in% impcolnames)
# keep important columns in training data frame
training &lt;- training[,impcolindex]
</code></pre>

<p>Now that we have reduced the predictors significantly from 160 to 12 predictors we will fit a more complex machine learning algorithm based on random forest. Additionally we will use cross validation to minimize in order to get a more realistic out of sample error rate.</p>

<pre><code class="r">fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;,number = 10, repeats = 10)
preObj &lt;- preProcess(training,method=c(&quot;knnImpute&quot;))
trainingImputed &lt;- predict(preObj,newdata=training)
modFit &lt;- train(trainclasse ~.,data=trainingImputed,
                method=&quot;rf&quot;
                #method=&quot;rpart&quot;
              ,trControl = fitControl
              )
# print summary of model
modFit
</code></pre>

<pre><code>## Random Forest 
## 
## 14718 samples
##    11 predictors
##     5 classes: &#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## 
## Summary of sample sizes: 13246, 13247, 13248, 13246, 13245, 13246, ... 
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
##    2    0.9896725  0.9869375  0.002669353  0.003376401
##    7    0.9948226  0.9934509  0.001938324  0.002452135
##   12    0.9871313  0.9837225  0.003111806  0.003936557
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 7.
</code></pre>

<p>Now let us compare the in sample error versus the out of sample error. Therefore let&#39;s compute the out of sample error by predicting based on our model fit and the testing data set.</p>

<pre><code class="r"># subselect the predictors used for training
traincolnames &lt;- colnames(training)
traincolindex &lt;- which(names(testing) %in% traincolnames)
testing &lt;- testing[,traincolindex]
preObj &lt;- preProcess(testing,method=c(&quot;knnImpute&quot;))
testingImputed &lt;- predict(preObj,newdata=testing)
 predictions &lt;- predict(modFit,newdata=testingImputed)
</code></pre>

<p>Now we would like to compare the in sample error and the out of sample error.</p>

<pre><code class="r"># the in sample accuracy of the prediction model
 inSampleError.accuracy &lt;- max(modFit$results[,&quot;Accuracy&quot;])
inSampleError.accuracy
</code></pre>

<pre><code>## [1] 0.9948226
</code></pre>

<pre><code class="r"># out of sample error and percentage of out of sample error
 inSampleError &lt;- 1 - inSampleError.accuracy
 inSampleError
</code></pre>

<pre><code>## [1] 0.005177404
</code></pre>

<pre><code class="r"># out of sample accuracy of the predicted model
 outOfSampleError.accuracy &lt;- sum(predictions == testingclasse)/length(predictions)
 outOfSampleError.accuracy
</code></pre>

<pre><code>## [1] 0.8380914
</code></pre>

<pre><code class="r"># out of sample error and percentage of out of sample error
 outOfSampleError &lt;- 1 - outOfSampleError.accuracy
 outOfSampleError
</code></pre>

<pre><code>## [1] 0.1619086
</code></pre>

<p>The in sample error based on training with cross validation is 0.0051774 whereas the out of sample error based on the seperate test data frame is 0.1619086 .</p>

</body>

</html>
